{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the re-partitioned file \n",
    "read_sendsticker = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/repartioned_data/*/*/*/*.parquet\")\n",
    "#dropduplicates\n",
    "read_sendsticker = read_sendsticker.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split a single column to multiple columns\n",
    "def extract_json(line, schema_names):\n",
    "    result = [x for i,x in enumerate(line) if i!=4]\n",
    "    properties = json.loads(line[4])\n",
    "    flattened = []     \n",
    "    for key in schema_names:\n",
    "        if key in properties:\n",
    "            if type(properties[key]) == unicode :\n",
    "                flattened.append(properties[key].encode('utf-8'))\n",
    "            else:\n",
    "                flattened.append(str(properties[key]))\n",
    "                \n",
    "        else:\n",
    "            flattened.append(\"NULL\")\n",
    "    result = list(result) + flattened\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_send = spark.read.json(read_sendsticker.rdd.map(lambda row: row.properties)).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the new columns for the file \n",
    "old_column_names_send = read_sendsticker.drop('properties').columns\n",
    "new_items_send = ['user_id_n' if x == 'user_id' else 'app_version_n' if  x == 'app_version' else 'city_n' if  x == 'city' else 'connection_n' if  x == 'connection' else 'country_n' if  x == 'country' else 'device_brand_n' if  x == 'device_brand' else 'device_model_n' if  x == 'device_model' else 'has_nfc_n' if  x == 'has_nfc' else 'height_resolution_n' if  x == 'height_resolution' else 'new_user_n' if  x == 'new_user' else 'new_user_n_1' if  x == 'new_User' else 'os_version_n' if  x == 'os_version' else 'width_resolution_n' if  x == 'width_resolution' else x for x in json_schema_send.names]\n",
    "fin_col_names_send = old_column_names_send + new_items_send\n",
    "df = read_sendsticker.rdd.map(lambda x: extract_json(x, json_schema_send.names)).toDF(fin_col_names_send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deriving new columns\n",
    "df = df.withColumn('City',when(df.City == 'NULL' ,df.city_n).otherwise(df.City))\n",
    "df = df.drop('city_n')\n",
    "df = df.withColumn('App_Version',when(df.App_Version == 'NULL' ,df.app_version_n).otherwise(df.App_Version))\n",
    "df = df.drop('app_version_n')\n",
    "df = df.withColumn('Connection',when(df.Connection == 'NULL' ,df.connection_n).otherwise(df.Connection))\n",
    "df = df.drop('connection_n')\n",
    "df = df.withColumn('Country',when(df.Country == 'NULL' ,df.country_n).otherwise(df.Country))\n",
    "df = df.drop('country_n')\n",
    "df = df.withColumn('Device_Brand',when(df.Device_Brand == 'NULL' ,df.device_brand_n).otherwise(df.Device_Brand))\n",
    "df = df.drop('device_brand_n')\n",
    "df = df.withColumn('Device_Model',when(df.Device_Model == 'NULL' ,df.device_model_n).otherwise(df.Device_Model))\n",
    "df = df.drop('device_model_n')\n",
    "df = df.withColumn('Has_NFC',when(df.Has_NFC == 'NULL' ,df.has_nfc_n).otherwise(df.Has_NFC))\n",
    "df = df.drop('has_nfc_n')\n",
    "df = df.withColumn('Height_Resolution',when(df.Height_Resolution == 'NULL' ,df.height_resolution_n).otherwise(df.Height_Resolution))\n",
    "df = df.drop('height_resolution_n')\n",
    "df = df.withColumn('New_User',when(df.New_User == 'NULL' ,df.new_user_n).otherwise(df.New_User))\n",
    "df = df.withColumn('New_User',when(df.New_User == 'NULL' ,df.new_user_n_1).otherwise(df.New_User))\n",
    "df = df.drop('new_user_n_1')\n",
    "df = df.drop('new_user_n')\n",
    "df = df.withColumn('OS_Version',when(df.OS_Version == 'NULL' ,df.os_version_n).otherwise(df.OS_Version))\n",
    "df = df.drop('os_version_n')\n",
    "df = df.withColumn('Width_Resolution',when(df.Width_Resolution == 'NULL' ,df.width_resolution_n).otherwise(df.Width_Resolution))\n",
    "df = df.drop('width_resolution_n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o543.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(eco_id#380, sticker_pack_id#392, name#354, carrier#373, sticker_id#391, Country#720, c_pin#372, Width_Resolution#1369, App_Version#538, Height_Resolution#1060, Connection#630, long#386, Has_NFC#978, u_pin#394, City#444, app_name#370, event_action#381, OS_Version#1295, time#355, platform#390, New_User#1180, license_type#385, Device_Brand#808, test_environment#393, ... 5 more fields)\n+- *HashAggregate(keys=[eco_id#380, sticker_pack_id#392, name#354, carrier#373, sticker_id#391, Country#720, c_pin#372, Width_Resolution#1369, App_Version#538, Height_Resolution#1060, Connection#630, long#386, Has_NFC#978, u_pin#394, City#444, app_name#370, event_action#381, OS_Version#1295, time#355, platform#390, New_User#1180, license_type#385, Device_Brand#808, test_environment#393, ... 4 more fields], functions=[], output=[eco_id#380, sticker_pack_id#392, name#354, carrier#373, sticker_id#391, Country#720, c_pin#372, Width_Resolution#1369, App_Version#538, Height_Resolution#1060, Connection#630, long#386, Has_NFC#978, u_pin#394, City#444, app_name#370, event_action#381, OS_Version#1295, time#355, platform#390, New_User#1180, license_type#385, Device_Brand#808, test_environment#393, ... 4 more fields])\n   +- *Project [name#354, time#355, CASE WHEN (App_Version#356 = NULL) THEN app_version_n#371 ELSE App_Version#356 END AS App_Version#538, CASE WHEN (City#357 = NULL) THEN city_n#374 ELSE City#357 END AS City#444, CASE WHEN (Connection#358 = NULL) THEN connection_n#375 ELSE Connection#358 END AS Connection#630, CASE WHEN (Country#359 = NULL) THEN country_n#376 ELSE Country#359 END AS Country#720, CASE WHEN (Device_Brand#360 = NULL) THEN device_brand_n#377 ELSE Device_Brand#360 END AS Device_Brand#808, CASE WHEN (Device_Model#361 = NULL) THEN device_model_n#378 ELSE Device_Model#361 END AS Device_Model#894, CASE WHEN (Has_NFC#362 = NULL) THEN has_nfc_n#382 ELSE Has_NFC#362 END AS Has_NFC#978, CASE WHEN (Height_Resolution#363 = NULL) THEN height_resolution_n#383 ELSE Height_Resolution#363 END AS Height_Resolution#1060, CASE WHEN (CASE WHEN (New_User#364 = NULL) THEN new_user_n#388 ELSE New_User#364 END = NULL) THEN new_user_n_1#387 ELSE CASE WHEN (New_User#364 = NULL) THEN new_user_n#388 ELSE New_User#364 END END AS New_User#1180, CASE WHEN (OS_Version#365 = NULL) THEN os_version_n#389 ELSE OS_Version#365 END AS OS_Version#1295, CASE WHEN (Width_Resolution#368 = NULL) THEN width_resolution_n#396 ELSE Width_Resolution#368 END AS Width_Resolution#1369, action#369, app_name#370, c_pin#372, carrier#373, eco_id#380, event_action#381, lat#384, license_type#385, long#386, platform#390, sticker_id#391, ... 4 more fields]\n      +- Scan ExistingRDD[id#351,visit_id#352,user_id#353,name#354,time#355,App_Version#356,City#357,Connection#358,Country#359,Device_Brand#360,Device_Model#361,Has_NFC#362,Height_Resolution#363,New_User#364,OS_Version#365,UAdth_ResoD崩on\u0000\u0000\u0000\u0000#366,UAdth_resoD崩on\u0000\u0000\u0000\u0000#367,Width_Resolution#368,action#369,app_name#370,app_version_n#371,c_pin#372,carrier#373,city_n#374,... 22 more fields]\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.CoalesceExec.doExecute(basicPhysicalOperators.scala:582)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:180)\n\t... 45 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$.needToCopyObjectsBeforeShuffle(ShuffleExchange.scala:162)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$.prepareShuffleDependency(ShuffleExchange.scala:246)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 71 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f1b6869719e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m  \u001b[0;34m'test_environment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m  \u001b[0;34m'u_pin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m  'user_id_n').dropDuplicates().coalesce(32).write.mode('overwrite').parquet(\"gs://ds-url-catag/plenty_stickers_data/aggregate_data/\")\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31m<type 'str'>\u001b[0m: (<type 'exceptions.UnicodeEncodeError'>, UnicodeEncodeError('ascii', u'An error occurred while calling o543.parquet.\\n: org.apache.spark.SparkException: Job aborted.\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\\n\\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\\n\\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\\n\\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:280)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\\n\\tat java.lang.Thread.run(Thread.java:748)\\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\\nExchange hashpartitioning(eco_id#380, sticker_pack_id#392, name#354, carrier#373, sticker_id#391, Country#720, c_pin#372, Width_Resolution#1369, App_Version#538, Height_Resolution#1060, Connection#630, long#386, Has_NFC#978, u_pin#394, City#444, app_name#370, event_action#381, OS_Version#1295, time#355, platform#390, New_User#1180, license_type#385, Device_Brand#808, test_environment#393, ... 5 more fields)\\n+- *HashAggregate(keys=[eco_id#380, sticker_pack_id#392, name#354, carrier#373, sticker_id#391, Country#720, c_pin#372, Width_Resolution#1369, App_Version#538, Height_Resolution#1060, Connection#630, long#386, Has_NFC#978, u_pin#394, City#444, app_name#370, event_action#381, OS_Version#1295, time#355, platform#390, New_User#1180, license_type#385, Device_Brand#808, test_environment#393, ... 4 more fields], functions=[], output=[eco_id#380, sticker_pack_id#392, name#354, carrier#373, sticker_id#391, Country#720, c_pin#372, Width_Resolution#1369, App_Version#538, Height_Resolution#1060, Connection#630, long#386, Has_NFC#978, u_pin#394, City#444, app_name#370, event_action#381, OS_Version#1295, time#355, platform#390, New_User#1180, license_type#385, Device_Brand#808, test_environment#393, ... 4 more fields])\\n   +- *Project [name#354, time#355, CASE WHEN (App_Version#356 = NULL) THEN app_version_n#371 ELSE App_Version#356 END AS App_Version#538, CASE WHEN (City#357 = NULL) THEN city_n#374 ELSE City#357 END AS City#444, CASE WHEN (Connection#358 = NULL) THEN connection_n#375 ELSE Connection#358 END AS Connection#630, CASE WHEN (Country#359 = NULL) THEN country_n#376 ELSE Country#359 END AS Country#720, CASE WHEN (Device_Brand#360 = NULL) THEN device_brand_n#377 ELSE Device_Brand#360 END AS Device_Brand#808, CASE WHEN (Device_Model#361 = NULL) THEN device_model_n#378 ELSE Device_Model#361 END AS Device_Model#894, CASE WHEN (Has_NFC#362 = NULL) THEN has_nfc_n#382 ELSE Has_NFC#362 END AS Has_NFC#978, CASE WHEN (Height_Resolution#363 = NULL) THEN height_resolution_n#383 ELSE Height_Resolution#363 END AS Height_Resolution#1060, CASE WHEN (CASE WHEN (New_User#364 = NULL) THEN new_user_n#388 ELSE New_User#364 END = NULL) THEN new_user_n_1#387 ELSE CASE WHEN (New_User#364 = NULL) THEN new_user_n#388 ELSE New_User#364 END END AS New_User#1180, CASE WHEN (OS_Version#365 = NULL) THEN os_version_n#389 ELSE OS_Version#365 END AS OS_Version#1295, CASE WHEN (Width_Resolution#368 = NULL) THEN width_resolution_n#396 ELSE Width_Resolution#368 END AS Width_Resolution#1369, action#369, app_name#370, c_pin#372, carrier#373, eco_id#380, event_action#381, lat#384, license_type#385, long#386, platform#390, sticker_id#391, ... 4 more fields]\\n      +- Scan ExistingRDD[id#351,visit_id#352,user_id#353,name#354,time#355,App_Version#356,City#357,Connection#358,Country#359,Device_Brand#360,Device_Model#361,Has_NFC#362,Height_Resolution#363,New_User#364,OS_Version#365,UAdth_ResoD\\u5d29on\\x00\\x00\\x00\\x00#366,UAdth_resoD\\u5d29on\\x00\\x00\\x00\\x00#367,Width_Resolution#368,action#369,app_name#370,app_version_n#371,c_pin#372,carrier#373,city_n#374,... 22 more fields]\\n\\n\\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\\n\\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:115)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\\n\\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:252)\\n\\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\\n\\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:386)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\\n\\tat org.apache.spark.sql.execution.CoalesceExec.doExecute(basicPhysicalOperators.scala:582)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\\n\\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:180)\\n\\t... 45 more\\nCaused by: java.lang.NullPointerException\\n\\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$.needToCopyObjectsBeforeShuffle(ShuffleExchange.scala:162)\\n\\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$.prepareShuffleDependency(ShuffleExchange.scala:246)\\n\\tat org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)\\n\\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:124)\\n\\tat org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:115)\\n\\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\\n\\t... 71 more\\n', 7029, 7030, 'ordinal not in range(128)'))"
     ]
    }
   ],
   "source": [
    "#write to new files after processing \n",
    "df.select('id',\n",
    " 'visit_id',\n",
    " 'user_id'\n",
    " 'name',\n",
    " 'time',\n",
    " 'App_Version',\n",
    " 'City',\n",
    " 'Connection',\n",
    " 'Country',\n",
    " 'Device_Brand',\n",
    " 'Device_Model',\n",
    " 'Has_NFC',\n",
    " 'Height_Resolution',\n",
    " 'New_User',\n",
    " 'OS_Version',\n",
    " 'Width_Resolution',\n",
    " 'action',\n",
    " 'app_name',\n",
    " 'c_pin',\n",
    " 'carrier',\n",
    " 'eco_id',\n",
    " 'event_action',\n",
    " 'lat',\n",
    " 'license_type',\n",
    " 'long',\n",
    " 'platform',\n",
    " 'sticker_id',\n",
    " 'sticker_pack_id',\n",
    " 'test_environment',\n",
    " 'u_pin',\n",
    " 'user_id_n').dropDuplicates().coalesce(32).write.mode('overwrite').parquet(\"gs://ds-url-catag/plenty_stickers_data/aggregate_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
