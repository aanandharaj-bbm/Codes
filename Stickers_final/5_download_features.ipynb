{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_datasource(data_source):\n",
    "    path = \"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/aggregate_data/\"\n",
    "    df_new = spark.read.parquet(path)\n",
    "    df_new = df_new.withColumn('date', concat(df_new.time.substr(1, 10)))\n",
    "    df_new = df_new.withColumn('month', concat(df_new.time.substr(4, 5)))\n",
    "    return df_new\n",
    "\n",
    "#number of days logged in  \n",
    "def get_numofdays(dataframe,feature_concat):\n",
    "    new_col_name = feature_concat+\"_sent_nofdays\"\n",
    "    distinctdate = dataframe.groupby('user_id_n').agg(countDistinct('date').alias(\"nofdays\"))\n",
    "    new_distinctdate = distinctdate.selectExpr(\"user_id_n as user_id_n\",\"nofdays as \"+ new_col_name)\n",
    "    return new_distinctdate\n",
    "\n",
    "def get_date_list(dataframe):\n",
    "    dataframe_new = dataframe.select('user_id_n','date').dropDuplicates()\n",
    "    from pyspark.sql import functions as F\n",
    "    date_list = dataframe_new.groupby(\"user_id_n\").agg(F.collect_list(\"date\").alias(\"date_list\"))\n",
    "    return date_list\n",
    "\n",
    "def get_condays(read_data,data_source,feature_concat):\n",
    "    \n",
    "    #reading in the data\n",
    "    new_base = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/list_dates/\")\n",
    "\n",
    "    #computing the consecutive days \n",
    "    from pyspark.sql.types import FloatType,StringType\n",
    "    from datetime import datetime,timedelta\n",
    "    #function to derive average consecutive days\n",
    "    def con_days(raw):\n",
    "        raw = list(set(raw))\n",
    "        n = len(raw)\n",
    "        raw = sorted(raw, key=lambda d: map(int, d.split('-')))\n",
    "        no_of_days = 0\n",
    "        no_of_groups = 0\n",
    "        group = 0\n",
    "        total_days = 0\n",
    "        res = []\n",
    "        for i in range(0,n-1):\n",
    "            current_date = datetime.strptime(raw[i], \"%Y-%m-%d\")\n",
    "            current_date_mod = datetime.strftime(current_date, \"%Y-%m-%d\")\n",
    "            con_date = current_date + timedelta(days=1)\n",
    "            con_date_mod = datetime.strftime(con_date, \"%Y-%m-%d\")\n",
    "\n",
    "            next_date = datetime.strptime(raw[i+1], \"%Y-%m-%d\")\n",
    "            next_date_mod = datetime.strftime(next_date, \"%Y-%m-%d\")\n",
    "            if con_date_mod == next_date_mod :\n",
    "                no_of_days += 1\n",
    "                total_days += 1\n",
    "                if no_of_days == 1:\n",
    "                    group += 1\n",
    "                    no_of_groups = group\n",
    "\n",
    "            else:\n",
    "                no_of_days = 0\n",
    "        res.append(total_days)\n",
    "        return total_days\n",
    "\n",
    "    consecutive_days = udf(con_days,StringType())\n",
    "\n",
    "    def con_groups(raw):\n",
    "        raw = list(set(raw))\n",
    "        n = len(raw)\n",
    "        raw = sorted(raw, key=lambda d: map(int, d.split('-')))\n",
    "        no_of_days = 0\n",
    "        no_of_groups = 0\n",
    "        group = 0\n",
    "        total_days = 0\n",
    "        res = []\n",
    "        for i in range(0,n-1):\n",
    "            current_date = datetime.strptime(raw[i], \"%Y-%m-%d\")\n",
    "            current_date_mod = datetime.strftime(current_date, \"%Y-%m-%d\")\n",
    "            con_date = current_date + timedelta(days=1)\n",
    "            con_date_mod = datetime.strftime(con_date, \"%Y-%m-%d\")\n",
    "\n",
    "            next_date = datetime.strptime(raw[i+1], \"%Y-%m-%d\")\n",
    "            next_date_mod = datetime.strftime(next_date, \"%Y-%m-%d\")\n",
    "            if con_date_mod == next_date_mod :\n",
    "                no_of_days += 1\n",
    "                total_days += 1\n",
    "                if no_of_days == 1:\n",
    "                    group += 1\n",
    "                    no_of_groups = group\n",
    "\n",
    "            else:\n",
    "                no_of_days = 0\n",
    "        return no_of_groups\n",
    "\n",
    "    groups = udf(con_groups,StringType())\n",
    "    new_base = new_base.withColumn(\"Consective_days\",consecutive_days('date_list'))\n",
    "    new_base = new_base.withColumn(\"Consective_groups\",groups('date_list'))\n",
    "    new_base = new_base.withColumn(\"avg_con_days\",when((col('Consective_days') != 0)  & (col('Consective_groups') != 0) , round((col('Consective_days')/col('Consective_groups')))).otherwise(0))\n",
    "    final_base = new_base.select('user_id_n','avg_con_days')\n",
    "    new_col_name = feature_concat+\"_avg_con_days\"\n",
    "    final_base_one = final_base.selectExpr(\"user_id_n as user_id_n\",\"avg_con_days as \"+ new_col_name)\n",
    "    return final_base_one\n",
    "\n",
    "def get_stick_send(dataframe,feature_concat):\n",
    "    #sticker packs sent\n",
    "    stickers_sent = dataframe.select('user_id_n','sticker_pack_id').groupby('user_id_n').agg(count('sticker_pack_id').alias('sticker_packs'))\n",
    "    new_col_name = feature_concat+\"_sticker_packs\"\n",
    "    final_base_one = stickers_sent.selectExpr(\"user_id_n as user_id_n\",\"sticker_packs as \"+ new_col_name)\n",
    "    return final_base_one\n",
    "\n",
    "def get_stick_dist(dataframe,feature_concat):\n",
    "    #sticker packs sent\n",
    "    stickers_distinct_sent = dataframe.select('user_id_n','sticker_pack_id').groupby('user_id_n').agg(countDistinct('sticker_pack_id').alias('distinct_stick_packs'))\n",
    "    new_col_name = feature_concat+\"_distinct_stick_packs\"\n",
    "    final_base_one = stickers_distinct_sent.selectExpr(\"user_id_n as user_id_n\",\"distinct_stick_packs as \"+ new_col_name)\n",
    "    return final_base_one\n",
    "\n",
    "def get_stick_type(dataframe,feature_concat):\n",
    "    #reading in sticker type data\n",
    "    Sticker_types = spark.read.parquet(\"gs://ds-url-catag/Stickers/stick_bytype/agg_proc_stickertypes/*.parquet\")\n",
    "    Sticker_types = Sticker_types.dropDuplicates()\n",
    "\n",
    "    Sticker_categories = spark.read.parquet(\"gs://ds-url-catag/Stickers/processed_categories/*/*/*.parquet\")\n",
    "    Sticker_categories = Sticker_categories.dropDuplicates()\n",
    "    Sticker_categories = Sticker_categories.selectExpr(\"Sticker_id\", \"name\",\"animated\",\"Type as Type_n\",\"category\")\n",
    "\n",
    "    #joinging type and category data\n",
    "    data_join_type = dataframe.join(Sticker_types,Sticker_types.id==dataframe.sticker_pack_id,'left').select([dataframe.user_id_n,dataframe.sticker_pack_id]+[Sticker_types.type])\n",
    "    data_join_categories = data_join_type.join(Sticker_categories,Sticker_categories.Sticker_id==data_join_type.sticker_pack_id,'left').select([data_join_type.user_id_n,data_join_type.sticker_pack_id,data_join_type.type]+[Sticker_categories.name,Sticker_categories.animated,Sticker_categories.Type_n,Sticker_categories.category])\n",
    "    data_join_categories = data_join_categories.dropDuplicates()\n",
    "    #deriving new columns\n",
    "    data_join_categories = data_join_categories.withColumn('Final_Type',when(col('type').isNull(),col('Type_n')).otherwise(col('type')))\n",
    "    data_join_categories = data_join_categories.withColumn('Final_Type',when(col('Final_Type') == 'true','FREE').otherwise(col('Final_Type')))\n",
    "    #Deriving sticker type variables\n",
    "    #paid\n",
    "    data_join_categories  = data_join_categories.withColumn(\"sent_is_paid\",when(col('Final_Type') == 'PAID',data_join_categories.sticker_pack_id).otherwise(None))\n",
    "    data_join_categories  = data_join_categories.withColumn(\"sent_is_free\",when(col('Final_Type') == 'FREE',data_join_categories.sticker_pack_id).otherwise(None))\n",
    "    data_join_categories  = data_join_categories.withColumn(\"sent_is_subscribed\",when(col('Final_Type') == 'SUBSCRIPTION',data_join_categories.sticker_pack_id).otherwise(None))\n",
    "    data_join_categories  = data_join_categories.withColumn(\"sent_is_discontinued\",when(col('Final_Type') == 'DISCONTINUED',data_join_categories.sticker_pack_id).otherwise(None))\n",
    "\n",
    "    #replacing strings with numbers for clustering\n",
    "    data_join_categories = data_join_categories.withColumn('sent_is_paid',when(col('sent_is_paid').isNull(),0).otherwise(1))\n",
    "    data_join_categories = data_join_categories.withColumn('sent_is_free',when(col('sent_is_free').isNull(),0).otherwise(1))\n",
    "    data_join_categories = data_join_categories.withColumn('sent_is_subscribed',when(col('sent_is_subscribed').isNull(),0).otherwise(1))\n",
    "    data_join_categories = data_join_categories.withColumn('sent_is_discontinued',when(col('sent_is_discontinued').isNull(),0).otherwise(1))\n",
    "\n",
    "    #aggregating the data\n",
    "    more_stick_base = data_join_categories.groupBy('user_id_n').agg(func.sum('sent_is_paid').alias('sum_paid'),func.sum('sent_is_free').alias('sum_free'),func.sum('sent_is_subscribed').alias('sum_subs'),func.sum('sent_is_discontinued').alias('sum_discont'))\n",
    "    col_one =feature_concat+\"_sum_paid\"\n",
    "    col_two =feature_concat+\"_sum_free\"\n",
    "    col_three =feature_concat+\"_sum_subs\"\n",
    "    col_four = feature_concat+\"_sum_discont\"\n",
    "    final_base_one = more_stick_base.selectExpr(\"user_id_n as user_id_n\",\"sum_paid as \"+ col_one ,\"sum_free as \"+ col_two,\"sum_subs as \"+ col_three,\"sum_discont as \"+ col_four)\n",
    "    return final_base_one\n",
    "\n",
    "def get_result(data_source,dataframe):\n",
    "    print data_source\n",
    "    #reading in all the written data \n",
    "    one = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/distinctdays/\")\n",
    "    two = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/Avg_con_days/\")\n",
    "    three = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/stick_packs_sent/\")\n",
    "    four = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/dist_stick_sent/\")\n",
    "    five = spark.read.parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/sticker_type_cat/\")\n",
    "    #join all datasets\n",
    "    data_one = one.join(two,two.user_id_n == one.user_id_n).drop(two.user_id_n)\n",
    "    data_two = data_one.join(three,three.user_id_n == data_one.user_id_n).drop(three.user_id_n)\n",
    "    data_three = data_two.join(four,four.user_id_n == data_two.user_id_n).drop(four.user_id_n)\n",
    "    data_four = data_three.join(five,five.user_id_n == data_three.user_id_n).drop(five.user_id_n)\n",
    "    data_city = dataframe.groupby('user_id_n','City').count()\n",
    "    data_city = data_city.dropDuplicates()\n",
    "    data_five = data_four.join(data_city,data_city.user_id_n == data_four.user_id_n).drop(data_city.user_id_n)\n",
    "    new_data = data_five.repartition(200)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "#write the final file to all the data\n",
    "def writing_to_parquet(data_source,dataframe,location):\n",
    "    print \"writing\"\n",
    "    path =  \"gs://ds-url-catag/plenty_stickers_data/\"+data_source+\"/derived_features/temp/\"+location+\"/\"\n",
    "    print path \n",
    "    dataframe.dropDuplicates().write.mode('overwrite').parquet(path)  \n",
    "    return \"Data written\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing\n",
      "gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/derived_features/temp/distinctdays/\n",
      "writing\n",
      "gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/derived_features/temp/list_dates/\n",
      "writing\n",
      "gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/derived_features/temp/Avg_con_days/\n",
      "writing\n",
      "gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/derived_features/temp/dist_stick_sent/\n",
      "writing\n",
      "gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/derived_features/temp/sticker_type_cat/\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o398.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:213)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 297 in stage 39.0 failed 4 times, most recent failure: Lost task 297.3 in stage 39.0 (TID 23732, 10.29.88.21, executor 350): java.io.FileNotFoundException: File not found : gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/aggregate_data/part-00005-09050e8f-af69-4be2-80d0-76d306ea9177-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:174)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)\n\t... 45 more\nCaused by: java.io.FileNotFoundException: File not found : gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/aggregate_data/part-00005-09050e8f-af69-4be2-80d0-76d306ea9177-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:174)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ec3abe13556c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0msub_feature\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sticker_type_cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0msticktypecat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stick_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mwriting_to_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msticktypecat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sticker_type_cat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gs://ds-url-catag/plenty_stickers_data/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/derived_features/Final/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ad61ef697e67>\u001b[0m in \u001b[0;36mwriting_to_parquet\u001b[0;34m(data_source, dataframe, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m\"gs://ds-url-catag/plenty_stickers_data/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/derived_features/temp/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"Data written\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o398.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:213)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:166)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:508)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 297 in stage 39.0 failed 4 times, most recent failure: Lost task 297.3 in stage 39.0 (TID 23732, 10.29.88.21, executor 350): java.io.FileNotFoundException: File not found : gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/aggregate_data/part-00005-09050e8f-af69-4be2-80d0-76d306ea9177-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:174)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:186)\n\t... 45 more\nCaused by: java.io.FileNotFoundException: File not found : gs://ds-url-catag/plenty_stickers_data/event=BBM-STICKER-SEND/aggregate_data/part-00005-09050e8f-af69-4be2-80d0-76d306ea9177-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:174)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    Sticker_datasources = ['event=BBM-STICKER-SEND']\n",
    "    # values for Sticker_Datasources\n",
    "    # 'event=BBM-STICKER-RECEIVED','event=BBM-STICKER_SPONSORED_LINK-CLICK','event=BBM-STICKER-CLICK','event=BBM-STICKER-DOWNLOAD','event=BBM-STICKER-SEND'\n",
    "    for ds in Sticker_datasources:\n",
    "        split_name = ds.split('-')\n",
    "        if split_name[1] == \"STICKER_SPONSORED_LINK\": \n",
    "            feature_concat = \"sp_\"+split_name[2].lower()\n",
    "        else:\n",
    "            feature_concat = split_name[2].lower()\n",
    "        read_data = reading_datasource(ds)\n",
    "        features =['distinctdays','list_dates','Avg_con_days','dist_stick_sent','sticker_type_cat']\n",
    "        #values for features \n",
    "        #'distinctdays','list_dates','Avg_con_days','stick_packs_sent','dist_stick_sent','sticker_type_cat']\n",
    "        for sub_feature in features:\n",
    "            if sub_feature == 'distinctdays':\n",
    "                numofdays = get_numofdays(read_data,feature_concat)     \n",
    "                writing_to_parquet(ds,numofdays,'distinctdays')\n",
    "            elif sub_feature == 'list_dates':\n",
    "                datelist = get_date_list(read_data)          \n",
    "                writing_to_parquet(ds,datelist,'list_dates')\n",
    "            elif sub_feature == 'Avg_con_days':\n",
    "                avgcondays = get_condays(read_data,ds,feature_concat)          \n",
    "                writing_to_parquet(ds,avgcondays,'Avg_con_days')\n",
    "            elif sub_feature == 'stick_packs_sent':\n",
    "                sticksent = get_stick_send(read_data,feature_concat)          \n",
    "                writing_to_parquet(ds,sticksent,'stick_packs_sent')\n",
    "            elif sub_feature == 'dist_stick_sent':\n",
    "                stickdistsent = get_stick_dist(read_data,feature_concat)          \n",
    "                writing_to_parquet(ds,stickdistsent,'dist_stick_sent')\n",
    "            elif sub_feature == 'sticker_type_cat':\n",
    "                sticktypecat = get_stick_type(read_data,feature_concat)      \n",
    "                writing_to_parquet(ds,sticktypecat,'sticker_type_cat')\n",
    "        result = get_result(ds,read_data)\n",
    "        result.dropDuplicates().write.mode('overwrite').parquet(\"gs://ds-url-catag/plenty_stickers_data/\"+ds+\"/derived_features/Final/\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
