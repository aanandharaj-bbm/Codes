{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in sticker type data\n",
    "feature_data = spark.read.parquet(\"gs://ds-url-catag/Stickers/stick_statistics/sticker_download/derived_features/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert pyspark to pandas dataframe\n",
    "import pandas as pd\n",
    "feature_data_df  = feature_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the dataframe: (2422112, 9)\n"
     ]
    }
   ],
   "source": [
    "#checking size of dataframe\n",
    "print 'Size of the dataframe: {}'.format(feature_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logarithmic transformation of two different variables\n",
    "import numpy as np\n",
    "feature_data_df['log_noofdays']=np.log10(1+feature_data_df.numofdays)\n",
    "feature_data_df['log_avg_con_days']=np.log10(1+feature_data_df.avg_con_days)\n",
    "feature_data_df['log_sticker_packs_download']=np.log10(1+feature_data_df.sticker_packs_download)\n",
    "feature_data_df['log_distinct_sticker_packs_download']=np.log10(1+feature_data_df.distinct_sticker_packs_download)\n",
    "feature_data_df['log_sum_paid']=np.log10(1+feature_data_df.sum_paid)\n",
    "feature_data_df['log_sum_free']=np.log10(1+feature_data_df.sum_free)\n",
    "feature_data_df['log_sum_subs']=np.log10(1+feature_data_df.sum_subs)\n",
    "feature_data_df['log_sum_discont']=np.log10(1+feature_data_df.sum_discont)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [ 0.84976502  0.06767649  0.03883627]\n"
     ]
    }
   ],
   "source": [
    "#computing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(feature_data_df[['log_noofdays','log_avg_con_days','log_sticker_packs_download','log_distinct_sticker_packs_download','log_sum_paid','log_sum_free','log_sum_subs','log_sum_discont']].values)\n",
    "\n",
    "feature_data_df['pca-one'] = pca_result[:,0]\n",
    "feature_data_df['pca-two'] = pca_result[:,1] \n",
    "feature_data_df['pca-three'] = pca_result[:,2]\n",
    "\n",
    "\n",
    "\n",
    "print 'Explained variation per principal component: {}'.format(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "model = kmeans.fit(feature_data_df[['pca-one','pca-two','pca-three']])\n",
    "y_kmeans = kmeans.predict(feature_data_df[['pca-one','pca-two','pca-three']])\n",
    "centers = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_df['clusters'] =  kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = sqlContext.createDataFrame(feature_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = original_data.select('user_id_n','numofdays','avg_con_days','sticker_packs_download','distinct_sticker_packs_download','sum_paid','sum_free','sum_subs','sum_discont','clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|       avg_con_days|\n",
      "+-------+-------------------+\n",
      "|  count|            2422112|\n",
      "|   mean|0.09567270217066759|\n",
      "| stddev|0.34783412371311795|\n",
      "|    min|                0.0|\n",
      "|    max|               48.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_data.select('avg_con_days').describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average sticker pack download to distinguish the clusters \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import percent_rank, col\n",
    "window = Window.partitionBy('clusters')\n",
    "\n",
    "main_data = main_data.withColumn('Avg_stick_download',avg(col('sticker_packs_download')).over(window))\n",
    "\n",
    "#categorize ability to pay\n",
    "main_data = main_data.withColumn('Abilty_to_pay',when(col('Avg_stick_download') > 43.0,'Very High').otherwise(when(col('Avg_stick_download') > 7.0,'High').otherwise(when(col('Avg_stick_download') > 3.0,'Medium').otherwise(when(col('Avg_stick_download') > 1.4,'low').otherwise('very low')))))\n",
    "\n",
    "\n",
    "#get the percentiles for free,paid and subscribed among the clusters\n",
    "#usage rate of free,paid and subscribed\n",
    "main_data = main_data.withColumn('Freestickerusagerate',col('sum_free')/col('distinct_sticker_packs_download'))\n",
    "main_data = main_data.withColumn('paidstickerusagerate',col('sum_paid')/col('distinct_sticker_packs_download'))\n",
    "main_data = main_data.withColumn('subsstickerusagerate',col('sum_subs')/col('distinct_sticker_packs_download'))\n",
    "\n",
    "\n",
    "\n",
    "# #calculate the top percent rank for users performing better in the same cluster \n",
    "window_freerank = Window.partitionBy('Abilty_to_pay').orderBy(main_data['Freestickerusagerate'].desc())\n",
    "main_data = main_data.withColumn('Free_Stickers_rank',percent_rank().over(window_freerank).alias('rank'))\n",
    "\n",
    "window_paidrank = Window.partitionBy('Abilty_to_pay').orderBy(main_data['paidstickerusagerate'].desc())\n",
    "main_data = main_data.withColumn('Paid_Stickers_rank',percent_rank().over(window_freerank).alias('rank'))\n",
    "\n",
    "window_subsrank = Window.partitionBy('Abilty_to_pay').orderBy(main_data['subsstickerusagerate'].desc())\n",
    "main_data = main_data.withColumn('Subscribed_Stickers_rank',percent_rank().over(window_freerank).alias('rank'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.groupby('Free_sticker_usage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = main_data.withColumn('Free_sticker_usage',when(col('Free_Stickers_rank') <= 0.2,'High').otherwise(when( (col('Free_Stickers_rank') > 0.2) & (col('Free_Stickers_rank') < 0.5),'medium').otherwise('low')))\n",
    "main_data = main_data.withColumn('paid_sticker_usage',when(col('Paid_Stickers_rank') <= 0.2,'High').otherwise(when( (col('Paid_Stickers_rank') > 0.2) & (col('Paid_Stickers_rank') < 0.5),'medium').otherwise('low')))\n",
    "main_data = main_data.withColumn('Subscribed_sticker_usage',when(col('Subscribed_Stickers_rank') <= 0.2,'High').otherwise(when( (col('Subscribed_Stickers_rank') > 0.2) & (col('Subscribed_Stickers_rank') < 0.5),'medium').otherwise('low')))\n",
    "main_data = main_data.withColumn('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+------+\n",
      "|Abilty_to_pay|paid_sticker_usage| count|\n",
      "+-------------+------------------+------+\n",
      "|         High|              High| 77300|\n",
      "|         High|            medium| 93278|\n",
      "|         High|               low|167365|\n",
      "|          low|              High|484992|\n",
      "|    Very High|              High| 20894|\n",
      "|    Very High|            medium| 31439|\n",
      "|    Very High|               low| 52125|\n",
      "|     very low|              High|736838|\n",
      "|     very low|               low|151303|\n",
      "|       Medium|              High|138066|\n",
      "|       Medium|            medium|170949|\n",
      "|       Medium|               low|297563|\n",
      "+-------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_data.groupby('Abilty_to_pay','paid_sticker_usage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|       avg_con_days|\n",
      "+-------+-------------------+\n",
      "|  count|            2422112|\n",
      "|   mean|0.09567270217066759|\n",
      "| stddev|0.34783412371311795|\n",
      "|    min|                0.0|\n",
      "|    max|               48.0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_data.select('avg_con_days').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
